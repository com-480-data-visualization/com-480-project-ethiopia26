{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,\"../models/PyTorch_CIFAR10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_models import vgg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Torchlurk import Lurk\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from misc_funcs import clean_bw_imgs,sample_imagefolder,plot_hist,crop_imgs\n",
    "import jdc\n",
    "from matplotlib.pyplot import imshow\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import ToTensor,ToPILImage\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import pathlib\n",
    "import dill\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "from ImageFolderWithPaths import ImageFolderWithPaths\n",
    "from Projector import Projector\n",
    "\n",
    "#libraries\n",
    "sys.path.insert(1, '../lib/pytorch-cnn-visualizations/src/')\n",
    "from cnn_layer_visualization import CNNLayerVisualization\n",
    "from layer_activation_with_guided_backprop import GuidedBackprop\n",
    "from misc_functions import save_gradient_images\n",
    "from misc_funcs import create_folders,sample_imagefolder,create_labels\n",
    "from get_models import get_alex_places\n",
    "from time import perf_counter \n",
    "from enum import Enum\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = Enum(\n",
    "    value='State',\n",
    "    names=[\n",
    "        ('idle', 1),\n",
    "        ('compute top', 2),\n",
    "        ('compute_top', 2),\n",
    "        ('compute activ', 3),\n",
    "        ('compute_activ', 3),\n",
    "        ('compute grad', 4),\n",
    "        ('compute_grad', 4)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lurk():\n",
    "    def __init__(self,model,preprocess,labels_path,\n",
    "                 save_gen_imgs_dir,save_json_path,imgs_src_dir,\n",
    "                 n_top_avg=3,n_top_max=3,load_json_path=None,side_size=224):\n",
    "        \"\"\"\n",
    "        Lurker class: one lurker can be instanciated per trained pytorch network. Several methods allow to generate various types of data\n",
    "        concerning the network and can be visualized thanks to the bash command TOCOMPLETE\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : torch.model\n",
    "            the trained model\n",
    "        preprocess : torchvision.transforms.transforms.Compose\n",
    "            the preprocessing used when training the model\n",
    "        labels_path : string\n",
    "            path to the file associating the class titles (ex: penguin) to their dirname (ex:n02018795) and their label (ex:724)\n",
    "        save_gen_imgs_dir : str\n",
    "            directory where to save the generated images: some subdirectories will be created inside\n",
    "        save_json_path : string\n",
    "            path to the json object to create which will stock information for subsequent visualization. Must have an \".json\" extension\n",
    "        imgs_src_dir : string\n",
    "            directory to the train set of the model\n",
    "        n_top_avg : int\n",
    "            how many average images to keep in the top average filter activations\n",
    "        n_top_max : int\n",
    "            how many average images to keep in the top maximum filter activations\n",
    "        load_json_path : string\n",
    "            path to a previously computed json file: NB the other arguments need to be similar as during this first run. If set to a value,\n",
    "            the lurker will load the previously computed informations, if set to None, will start a new lurker from scratch\n",
    "            \n",
    "        \"\"\"\n",
    "        ##################### TODO: get rid of dev#####################\n",
    "        # allow to run reduced computations\n",
    "        self.DEVELOPMENT = True\n",
    "        #number of layers we compute stuff for in development mode\n",
    "        self.N_LAYERS_DEV = 1\n",
    "        #number of filters we compute stuff for in development mode\n",
    "        self.N_FILTERS_DEV = 1\n",
    "        ###############################################################\n",
    "        \n",
    "        #model to compute a lurker for\n",
    "        self.model = model\n",
    "        #preprocessing used when training the model\n",
    "        self.preprocess = preprocess\n",
    "        self.side_size = side_size\n",
    "        #state of the lurker: takes values in \"idle\",\"compute top\",\"compute activ\",\"compute grad\"\n",
    "        self.state = State.idle\n",
    "\n",
    "        #path to the labels\n",
    "        self.LABELS_PATH = labels_path\n",
    "        #directory where to save the generated images\n",
    "        self.GEN_IMGS_DIR = save_gen_imgs_dir\n",
    "        #directory where to load the training images\n",
    "        self.IMGS_DIR = imgs_src_dir\n",
    "        \n",
    "        #where to save the json\n",
    "        self.JSON_PATH_WRITE = save_json_path\n",
    "        #number of avg spikes images per filter\n",
    "        self.N_TOP_AVG = n_top_avg\n",
    "        # number of max spikes images per filter\n",
    "        self.N_TOP_MAX = n_top_max\n",
    "        \n",
    "        #path to the numb image\n",
    "        self.NUMB_PATH = \"static/data/numb.png\"\n",
    "        \n",
    "        self.dataset = ImageFolderWithPaths(self.IMGS_DIR,transform=self.preprocess)\n",
    "        self.data_loader = torch.utils.data.DataLoader(self.dataset, batch_size=1, shuffle=True)\n",
    "        # each class has 3 kinds of representation: (1)class titles (ex: \"penguin\") (2) dirname (ex:\"n02018795\") (3) label (ex:724)\n",
    "        # CLASS2LAB and LAB2TITLE allow to convert the info from one type to the other\n",
    "        self.CLASS2LAB = self.dataset.class_to_idx\n",
    "        labels_infos = self.recreate_labels()\n",
    "        self.LAB2TITLE = labels_infos.set_index('label')['title'].to_dict()\n",
    "        \n",
    "        assert(set(self.LAB2TITLE.keys())==(set(self.CLASS2LAB.values())))\n",
    "        assert(len(self.LAB2TITLE) == len(self.CLASS2LAB))\n",
    "        \n",
    "        if load_json_path is not None:\n",
    "            # loading information from a previously computed json file\n",
    "            self.load_from_json(load_json_path)\n",
    "        else:\n",
    "            # building the information from scratch\n",
    "            self.__build_model_info()\n",
    "        self.save_to_json()\n",
    "        \n",
    "        create_folders(self.GEN_IMGS_DIR,[\"avg_act\",\"avg_act_grad\",\"max_act\",\"max_act_grad\",\"max_act_cropped\",\"max_act_cropped_grad\",\"filt_viz\"],self.model_info)\n",
    "        \n",
    "        \n",
    "        self.title_counts = dict(zip(self.LAB2TITLE.values(),[0] *len(self.LAB2TITLE)))\n",
    "        #initiate the number of counts for the classes\n",
    "        self.__init_class_counts(self.LAB2TITLE, self.IMGS_DIR,self.title_counts)\n",
    "    \n",
    "    \n",
    "    ################################ Building/Loading ################################\n",
    "    \n",
    "    def __build_model_info(self):\n",
    "        \"\"\"\n",
    "        build the model_info from scratch\n",
    "        \"\"\"\n",
    "        model_info = []\n",
    "        layers = []\n",
    "        #construct the data structure\n",
    "        for layer in list(self.model.features.named_children()):\n",
    "            lay_info = {'id':layer[0],\n",
    "                      'lay':layer[1],\n",
    "                      'name':str(layer[1]).split('(')[0] + \"_\" + str(layer[0]) \n",
    "                    }\n",
    "            if (isinstance(layer[1],(nn.Conv2d,nn.MaxPool2d))):\n",
    "                layers.append(layer[1])\n",
    "            if (isinstance(lay_info['lay'],nn.Conv2d)):     \n",
    "                n_input = lay_info['lay'].in_channels\n",
    "                n_output = lay_info['lay'].out_channels\n",
    "                lay_info['n_input'] = n_input\n",
    "                lay_info['n_output'] = n_output\n",
    "                lay_info['deproj'] = Projector(deepcopy(layers),224)\n",
    "                lay_info[\"filters\"] = []\n",
    "                for i in range(n_output):\n",
    "                    lay_info[\"filters\"].append({\n",
    "                        \"id\":i,\n",
    "                        \"avg_spikes\":[0 for i in range(self.N_TOP_AVG)],\n",
    "                        \"avg_imgs\":[self.NUMB_PATH for i in range(self.N_TOP_AVG)],\n",
    "                        \"avg_imgs_grad\":[self.NUMB_PATH for i in range(self.N_TOP_AVG)],\n",
    "                        \"max_spikes\":[0 for i in range(self.N_TOP_MAX)],\n",
    "                        \"max_slices\":[[[0,0],[0,0]]for i in range(self.N_TOP_MAX)],\n",
    "                        \"max_imgs\":[self.NUMB_PATH for i in range(self.N_TOP_MAX)],\n",
    "                        \"max_imgs_crop\":[self.NUMB_PATH for i in range(self.N_TOP_MAX)],\n",
    "                        \"max_imgs_grad\":[self.NUMB_PATH for i in range(self.N_TOP_MAX)],\n",
    "                        \"max_imgs_crop_grad\":[self.NUMB_PATH for i in range(self.N_TOP_MAX)],\n",
    "                        \"filter_viz\":self.NUMB_PATH,\n",
    "                        \"histo_counts_max\":OrderedDict(zip(self.LAB2TITLE.values(),[0] *len(self.LAB2TITLE))),\n",
    "                        \"histo_counts_avg\":OrderedDict(zip(self.LAB2TITLE.values(),[0] *len(self.LAB2TITLE)))\n",
    "                    })\n",
    "            elif (type(lay_info['lay']) == nn.Linear):\n",
    "                    n_input = lay_info['lay'].in_features\n",
    "                    n_output = lay_info['lay'].out_features\n",
    "                    lay_info['n_output'] = n_output\n",
    "                    #lay_info[\"filters\"] = [empty_filter.copy() for i in range(n_output)]\n",
    "            model_info.append(lay_info)\n",
    "            self.model_info = model_info\n",
    "            self.conv_layinfos = [lay_info for lay_info in self.model_info if isinstance(lay_info['lay'],nn.Conv2d)]\n",
    "            \n",
    "    def set_state(self,state):\n",
    "        self.state = state\n",
    "        with open(self.JSON_PATH_WRITE, 'r+') as f:\n",
    "            data = json.load(f)\n",
    "            data['state'] = self.state.name\n",
    "            f.seek(0)        \n",
    "            json.dump(data, f, indent=2)\n",
    "            f.truncate()\n",
    "\n",
    "    def save_to_json(self):\n",
    "        \"\"\"\n",
    "        save the json information of self.model_info\n",
    "        \"\"\"\n",
    "        model_info2 = deepcopy(self.model_info)\n",
    "        for lay_info in model_info2:\n",
    "            if (isinstance(lay_info['lay'],nn.Conv2d)):\n",
    "                del lay_info['deproj']\n",
    "            del lay_info['lay']\n",
    "        with open(self.JSON_PATH_WRITE, 'w') as fout:\n",
    "            model_info2 = {'state':self.state.name,'infos':model_info2}\n",
    "            json.dump(model_info2, fout, indent = 2)\n",
    "        print(\"json saving done!\")\n",
    "        \n",
    "    def load_from_json(self,load_path):\n",
    "        \"\"\"\n",
    "        load from json information\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        with open(load_path, 'r') as fin:\n",
    "            model_info = json.load(fin)['infos']\n",
    "        for lay_info,layer in zip(model_info,self.model.features):\n",
    "            lay_info['lay'] = layer\n",
    "            if (isinstance(layer,(nn.Conv2d,nn.MaxPool2d))):\n",
    "                layers.append(layer)\n",
    "            if (isinstance(layer,nn.Conv2d)):\n",
    "                lay_info['deproj'] = Projector(deepcopy(layers),224)\n",
    "        self.model_info = model_info\n",
    "        self.conv_layinfos = [lay_info for lay_info in self.model_info if isinstance(lay_info['lay'],nn.Conv2d)]\n",
    "        self.__check_imgs_exist()\n",
    "        print(\"Loading done!\") \n",
    "    \n",
    "        \n",
    "    def save_to_dill(self,path):\n",
    "        \"\"\"\n",
    "        save to dill format: similar to pickle format,but handle additional data formats\n",
    "        \"\"\"\n",
    "        torch.save(self,path, pickle_module=dill)\n",
    "        print(\"dill saving done!\")\n",
    "        \n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_dill(load_path,overwrite=False,alternate_json_path=None):\n",
    "        \"\"\"\n",
    "        load the lurker from a dill (pickle-like) file\n",
    "        Args:\n",
    "            load_path(str): path to the dill file\n",
    "            overwrite(Bool): whether to overwrite the JSON the lurker reads from\n",
    "            alternate_path(str): only valid if overwrite is set to False: specific path to the write json\n",
    "        \"\"\"\n",
    "        assert(not (overwrite and alternate_json_path is not None))\n",
    "        lurker = torch.load(load_path, pickle_module=dill)\n",
    "        if not overwrite:\n",
    "            if alternate_json_path is None:\n",
    "                path = Path(lurker.JSON_PATH_WRITE)\n",
    "                name = path.stem + \"_copy\"\n",
    "                lurker.JSON_PATH_WRITE = str(path.with_name(name).with_suffix(\".json\"))\n",
    "            else:\n",
    "                path = Path(alternate_json_path)\n",
    "                assert(path.is_file() and path.suffix == \".json\")\n",
    "                lurker.JSON_PATH_WRITE = alternate_json_path\n",
    "        lurker.__check_imgs_exist()\n",
    "        print(\"Loading done!\") \n",
    "        return lurker    \n",
    "    \n",
    "    def __check_imgs_exist(self): \n",
    "        \"\"\"\n",
    "        check that all the images in the json file indeed exist and are at the right position when loading\n",
    "        \"\"\"\n",
    "        try:\n",
    "            type_keys = [\"avg_imgs\",\"avg_imgs_grad\",\"max_imgs\",\"max_imgs_grad\"]\n",
    "            for lay_info in self.conv_layinfos:\n",
    "                for filtr in lay_info['filters']:\n",
    "                    for key in type_keys:\n",
    "                        for el in filtr[key]:\n",
    "                            Path(el).resolve(strict=True)\n",
    "                    Path(filtr[\"filter_viz\"]).resolve(strict=True)\n",
    "        except FileNotFoundError as e:\n",
    "            print(\"Non coherent path in the loaded object:{}\".format(e.filename))\n",
    "            \n",
    "    \n",
    "    ################################ Practical ################################\n",
    "\n",
    "            \n",
    "    def __get_filt_dir(self,dir_type,layer_name,filter_id):\n",
    "        \"\"\"\n",
    "        return the path to the appropriate folder\n",
    "        Parameters\n",
    "        ----------\n",
    "        dir_type : str\n",
    "            One of \"avg_act\",\"avg_act_grad\",\"max_act\",\"max_act_grad\",\"max_act_cropped\",\"max_act_cropped_grad\",\"filt_viz\"\n",
    "        layer_name : str\n",
    "            name of the layer\n",
    "        filter_id : int/str\n",
    "            index of the filter\n",
    "        \"\"\"\n",
    "        return os.path.join(self.GEN_IMGS_DIR,dir_type,str(layer_name),str(filter_id))\n",
    "    \n",
    "    def __extract_name(self,img_path,ext='.jpg'):\n",
    "        \"\"\"\n",
    "        extract the name of the imgpath and add the extension\n",
    "        ex:\n",
    "        >>> __extract_name(\"path/to/file.txt\")\n",
    "        return: file.jpg\n",
    "        \"\"\"\n",
    "        jpg_name = img_path.split(\"/\")[-1]\n",
    "        img_name = jpg_name.split(\".\")[0] + ext\n",
    "        return img_name\n",
    "    \n",
    "    ################################################################ Generated images ################################################################\n",
    "\n",
    "    ################################ average/maximum activation images ################################\n",
    "    def compute_top_imgs(self,verbose = False,compute_max=True,compute_avg=True,save_loc=False):\n",
    "        \"\"\"\n",
    "        compute the average and max images for all the layers of the model_info such that each filter of each layer knows what are\n",
    "        its favourite images (write down the link to the avg/max images in the json)\n",
    "        \"\"\"\n",
    "        self.set_state(State.compute_top)\n",
    "        t_start = perf_counter()  \n",
    "        assert(compute_max or compute_avg)\n",
    "        self.__reset_histos()\n",
    "        for j,(datas,labels,paths) in enumerate(self.data_loader):\n",
    "            print(\"Progression update favimgs:{:.2f} %\".format(j/len(self.data_loader) * 100))\n",
    "            for i,lay_info in enumerate(self.model_info):\n",
    "                clear_output(wait=True)\n",
    "                if verbose:\n",
    "                    print(\"AvgMax update:{}/{}:{:.2f} %..\".format(i,len(self.model_info),100*j/ len(data_loader)))\n",
    "\n",
    "                #datas: Batchsize x Numberfilter x Nout x Nout\n",
    "                datas = lay_info['lay'](datas)\n",
    "                if (not isinstance(lay_info['lay'],nn.Conv2d) ):\n",
    "                    continue\n",
    "                if (i >=self.N_LAYERS_DEV and self.DEVELOPMENT):\n",
    "                    break\n",
    "\n",
    "                batch_size = datas.size(0)\n",
    "                filter_nb = datas.size(1)\n",
    "                width = datas.size(3)\n",
    "\n",
    "                #spikes: Batchsize x Filternumber\n",
    "                max_spikes,max_pos = datas.view([batch_size,filter_nb,-1]).max(dim = 2)\n",
    "                max_rows = max_pos / width\n",
    "                max_cols = max_pos % width\n",
    "\n",
    "                avg_spikes = datas.view([batch_size,filter_nb,-1]).mean(dim = 2)\n",
    "                if compute_max:\n",
    "                    self.__update_filters_max_imgs(lay_info,max_spikes,paths,max_rows,max_cols,labels)\n",
    "                if compute_avg:\n",
    "                    self.__update_filters_avg_imgs(lay_info,avg_spikes,paths,labels)\n",
    "                #save the whole model\n",
    "                t_now = perf_counter()\n",
    "                if (t_now - t_start > 5):\n",
    "                    t_start = t_now\n",
    "                    self.save_to_json()\n",
    "\n",
    "        self.__normalize_histos()\n",
    "        self.__sort_filters_spikes()\n",
    "        if save_loc:\n",
    "            self.__save_avgmax_imgs()\n",
    "        self.__save_cropped()\n",
    "        self.set_state(State.idle)\n",
    "        self.save_to_json()\n",
    "\n",
    "        \n",
    "    def __save_avgmax_imgs(self):\n",
    "        \"\"\"\n",
    "        save the maximum average/maximum activation for each filter from the trainin set\n",
    "        \"\"\"\n",
    "        for lay_info in self.conv_layinfos:\n",
    "            for filtr in lay_info['filters']:\n",
    "                for agg in [\"avg\",\"max\"]:\n",
    "                    for i,src_path in enumerate(filtr[\"{}_imgs\".format(agg)]):\n",
    "                        new_dir = Path(self.__get_filt_dir(\"{}_act\".format(agg),lay_info[\"name\"],filtr[\"id\"]))\n",
    "                        name = Path(src_path).name\n",
    "                        new_path= new_dir.joinpath(name)\n",
    "                        #we open the path pointing to the training set\n",
    "                        im = Image.open(src_path)\n",
    "                        im.save(new_path)\n",
    "                        filtr[\"{}_imgs\".format(agg)][i] = new_path\n",
    "                    \n",
    "    def __sort_filters_spikes(self):\n",
    "        \"\"\"\n",
    "        sorts the spikes and respective paths of the filters inplace\n",
    "        \"\"\"\n",
    "        for lay_info in self.conv_layinfos:\n",
    "            for filtr in lay_info['filters']:\n",
    "                max_indx = np.argsort(filtr[\"max_spikes\"])[::-1]\n",
    "                filtr[\"max_spikes\"] = np.array(filtr[\"max_spikes\"])[max_indx].tolist()\n",
    "                filtr[\"max_imgs\"] = np.array(filtr[\"max_imgs\"])[max_indx].tolist()\n",
    "                filtr[\"max_slices\"] = np.array(filtr[\"max_slices\"])[max_indx].tolist()\n",
    "\n",
    "                avg_indx = np.argsort(filtr[\"avg_spikes\"])[::-1]\n",
    "                filtr[\"avg_spikes\"] = np.array(filtr[\"avg_spikes\"])[avg_indx].tolist()\n",
    "                filtr[\"avg_imgs\"] = np.array(filtr[\"avg_imgs\"])[avg_indx].tolist()\n",
    "    \n",
    "\n",
    "    def __update_filters_max_imgs(self,lay_info,batch_spikes,paths,max_rows,max_cols,labels):\n",
    "        #as many spikes in batch_spikes as there are samples in batch\n",
    "        for spikes,path,label,rows,cols in zip(batch_spikes,paths,labels,max_rows,max_cols):\n",
    "            #at this stage there are as many spike in spikes as there are filters\n",
    "            for k,(filt,spike,row,col) in enumerate(zip(lay_info[\"filters\"],spikes.detach().numpy(),rows,cols)):\n",
    "                #compute the histogram with maximal values\n",
    "                filt[\"histo_counts_max\"][self.LAB2TITLE[label.item()]] += float(spike)\n",
    "                #compute the minimum spike for the filter\n",
    "                min_indx = np.argmin(filt[\"max_spikes\"])\n",
    "                min_spike = min(filt[\"max_spikes\"])\n",
    "                \n",
    "                if (spike > min_spike and not (path in filt[\"max_imgs\"])):\n",
    "                    ((x1,x2),(y1,y2)) = lay_info[\"deproj\"].chain(((row.item(),row.item()),(col.item(),col.item())))\n",
    "                    assert(isinstance(x1,int) and isinstance(x2,int) and isinstance(y1,int) and isinstance(y2,int))\n",
    "                    filt[\"max_slices\"][min_indx] = ((x1,x2),(y1,y2))\n",
    "                    filt[\"max_imgs\"][min_indx] = path\n",
    "                    filt[\"max_spikes\"][min_indx] = float(spike)\n",
    "                    \n",
    "    def __update_filters_avg_imgs(self,lay_info,batch_spikes,paths,labels):\n",
    "        #as many spikes in batch_spikes as there are samples in batch\n",
    "        for spikes,path,label in zip(batch_spikes,paths,labels):\n",
    "            #at this stage there are as many spike in spikes as there are filters\n",
    "            for k,(filt,spike) in enumerate(zip(lay_info[\"filters\"],spikes.detach().numpy())):\n",
    "                #compute the histogram with avg values\n",
    "                filt[\"histo_counts_avg\"][self.LAB2TITLE[label.item()]] += float(spike)\n",
    "                #compute the minimum spike for the filter\n",
    "                min_indx = np.argmin(filt[\"avg_spikes\"])\n",
    "                min_spike = min(filt[\"avg_spikes\"])\n",
    "                if (spike > min_spike and not (path in filt[\"avg_imgs\"])):\n",
    "                    filt[\"avg_imgs\"][min_indx] = path\n",
    "                    filt[\"avg_spikes\"][min_indx] = float(spike)\n",
    "                \n",
    "\n",
    "    def __save_cropped(self,grad = False,verbose=False):\n",
    "        \"\"\"\n",
    "        iterate on the model_info to save a cropped version of the images\n",
    "        Args:\n",
    "            grad(Bool): whether to save the gradients versions\n",
    "        \"\"\"\n",
    "        ext = \"_grad\" if grad else \"\"\n",
    "        filtrlist = \"max_imgs{}\".format(ext)\n",
    "        folder = \"max_act_cropped{}\".format(ext)\n",
    "        filtrtargetlist = \"max_imgs_crop{}\".format(ext)\n",
    "\n",
    "        for i,lay_info in enumerate(self.conv_layinfos):\n",
    "            clear_output(wait=True)\n",
    "            if verbose:\n",
    "                print(\"Progression:{} %\".format(i/len(conv_layinfos)*100))\n",
    "            for filtr in lay_info['filters']:\n",
    "                for i,(src_path,slices) in enumerate(zip(filtr[filtrlist],filtr['max_slices'])):\n",
    "                    if (src_path == self.NUMB_PATH):\n",
    "                        continue\n",
    "                    src_path = Path(src_path)\n",
    "                    target_dir = Path(self.__get_filt_dir(folder,lay_info['name'],filtr['id']))\n",
    "                    file_name = src_path.name\n",
    "                    target_path = target_dir.joinpath(file_name)\n",
    "                    \n",
    "                    image = ToTensor()(Image.open(src_path))\n",
    "                    ((x1,x2),(y1,y2)) = slices\n",
    "                    cropped = image[:,x1:x2+1,y1:y2+1]\n",
    "                    ToPILImage()(cropped).save(str(target_path))\n",
    "                    filtr[filtrtargetlist][i] = str(target_path)\n",
    "                    \n",
    "    ################################ Histograms related ################################\n",
    "\n",
    "    def recreate_labels(self,output_path=None):\n",
    "        \"\"\"\n",
    "        recreate the label files wrt to a specific dataset with potentially less classes than the original one. \n",
    "        Useful for smaller computations\n",
    "        \"\"\"\n",
    "        #set_trace()\n",
    "        if output_path is None:\n",
    "            output_path = str(Path(self.IMGS_DIR).parent.joinpath(\"labels2.txt\"))\n",
    "        infos = pd.read_csv(self.LABELS_PATH,sep=\" \",header=None)\n",
    "        infos.columns = ['dir_name','label','title']\n",
    "        infos.set_index('dir_name',inplace=True,drop=False)\n",
    "        new_infos= infos.loc[self.dataset.class_to_idx.keys()].copy()\n",
    "        new_infos.label = new_infos.dir_name.map(self.dataset.class_to_idx)\n",
    "        new_infos.drop(columns=['dir_name']).to_csv(output_path,header=None,sep=\" \")\n",
    "        return new_infos\n",
    "            \n",
    "            \n",
    "    def __init_class_counts(self,indx_to_title,src_path,obj):\n",
    "        \"\"\"\n",
    "        create the dictionary which counts the number of images per classes in the dataset\n",
    "        \"\"\"\n",
    "        for subfold in os.listdir(src_path):\n",
    "            subfold_path = os.path.join(src_path,subfold)\n",
    "            count = len([name for name in os.listdir(subfold_path)])\n",
    "            title = indx_to_title[self.CLASS2LAB[subfold]]\n",
    "            obj[title] += count\n",
    "            \n",
    "    def __reset_histos(self):\n",
    "        \"\"\"\n",
    "        reset the counts for the histograms counts\n",
    "        \"\"\"\n",
    "        for lay_info in self.conv_layinfos:\n",
    "            for filt in lay_info['filters']:\n",
    "                filt['histo_counts_max'] = dict(zip(self.LAB2TITLE.values(),[0] *len(self.LAB2TITLE)))\n",
    "                filt['histo_counts_avg'] = dict(zip(self.LAB2TITLE.values(),[0] *len(self.LAB2TITLE)))\n",
    "            \n",
    "    def __normalize_histos(self):\n",
    "        \"\"\"\n",
    "        average the counts of the histograms wrt to the number of samples in the classes of the dataset\n",
    "        \"\"\"\n",
    "        for lay_info in self.conv_layinfos:\n",
    "            for filt in lay_info['filters']:\n",
    "                for key in filt['histo_counts_max'].keys():\n",
    "                    filt['histo_counts_max'][key] /= self.title_counts[key]\n",
    "                for key in filt['histo_counts_avg'].keys():\n",
    "                    filt['histo_counts_avg'][key] /= self.title_counts[key]\n",
    "                filt['histo_counts_max'] = OrderedDict(sorted(filt['histo_counts_max'].items(),key = lambda x: x[1])[::-1])\n",
    "                filt['histo_counts_avg'] =  OrderedDict(sorted(filt['histo_counts_avg'].items(),key = lambda x: x[1])[::-1])\n",
    "                \n",
    "\n",
    "                    \n",
    "    ################################ Layer visualizations ################################\n",
    "                    \n",
    "    def compute_viz(self,num_imgs_per_class=None,ratio_imgs_per_class=None):\n",
    "        \"\"\"\n",
    "        Compute the filter visualization for all classes\n",
    "        Args:\n",
    "            num_imgs_per_class(int): number of filters to compute the visualization for for each class\n",
    "            ratio_imgs_per_class(float): ratio of filters to compute the visualization for for each class\n",
    "        \"\"\"\n",
    "        self.set_state(State.compute_activ)\n",
    "        checks = [num_imgs_per_class is None , ratio_imgs_per_class is None]\n",
    "        assert(sum(checks)==1 or sum(checks)==0)\n",
    "        for lay_indx,lay_info in self.conv_layinfos:\n",
    "            print(\"Layer {}:\".format(lay_indx))\n",
    "            N = lay_info[\"lay\"].out_channels\n",
    "            indexes = np.arange(N)\n",
    "            np.random.shuffle(indexes)\n",
    "            if checks[0]:\n",
    "                indexes = indexes[:num_imgs_per_class]\n",
    "            elif checks[1]:\n",
    "                lim = int(ratio_imgs_per_class * N)\n",
    "                indexes = indexes[:lim]\n",
    "            self.compute_layer_viz(lay_indx,indexes)\n",
    "        self.set_state(State.idle)\n",
    "\n",
    "\n",
    "    def compute_layer_viz(self,layer_indx,filter_indexes = None):\n",
    "        \"\"\"\n",
    "        compute  and save the filter maximal activation as an image. Compute it only for filters for which\n",
    "        it has not been computed yet: you need to delete the existing image if you wish for a refresh.\n",
    "        Args:\n",
    "            layer_indx(int): layer to compute the filter for\n",
    "            filter_indexes(list(int)):  list of filter to compute the visualization for\n",
    "        \"\"\"\n",
    "        self.set_state(State.compute_activ)\n",
    "        lay_info = self.model_info[layer_indx]\n",
    "        if filter_indexes is None:\n",
    "            filter_indexes = range(lay_info[\"lay\"].out_channels)\n",
    "        else:\n",
    "            for i in filter_indexes:\n",
    "                if (i >= lay_info[\"lay\"].out_channels or i<0):\n",
    "                    raise IndexError(\"filter_indexes must have lower value than layer output number.\")\n",
    "        layer_name = lay_info[\"name\"]\n",
    "        pre_existing = []\n",
    "        for filt in lay_info[\"filters\"]:\n",
    "            name = \"{}_{}_max_activ.jpg\".format(lay_info['name'],filt['id'])\n",
    "            filt_path = self.__get_filt_dir(\"filt_viz\",lay_info[\"name\"],filt[\"id\"])\n",
    "            filt_path = os.path.join(filt_path,name)\n",
    "            try:\n",
    "                f = open(filt_path)\n",
    "                filt[\"filter_viz\"] = filt_path\n",
    "                pre_existing.append(filt[\"id\"])\n",
    "                f.close()\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "        filter_indexes = [i for i in filter_indexes if i not in pre_existing]\n",
    "\n",
    "        for j,filt_indx in enumerate(filter_indexes):\n",
    "            print(\"Filter {} / {}\".format(j+1,len(filter_indexes)))\n",
    "            filt = lay_info['filters'][filt_indx]\n",
    "            visualizer = CNNLayerVisualization(self.model.features, \n",
    "                                               selected_layer=int(lay_info['id']), \n",
    "                                               selected_filter=filt_indx,\n",
    "                                               side_size=self.side_size)\n",
    "            act_max_img = visualizer.visualise_layer_with_hooks()\n",
    "            filt_path = self.__get_filt_dir(\"filt_viz\",lay_info['name'],filt['id'])\n",
    "            name = \"{}_{}_max_activ.jpg\".format(lay_info['name'],filt['id'])\n",
    "            filt_path = os.path.join(filt_path,name)\n",
    "            #save the image\n",
    "            ToPILImage()(act_max_img).save(filt_path)\n",
    "            print(\"Vis saved!\")\n",
    "            filt[\"filter_viz\"] = filt_path\n",
    "            self.save_to_json()\n",
    "        print(\"Visualization done!\")\n",
    "        self.set_state(State.idle)\n",
    "        self.save_to_json()\n",
    "\n",
    "\n",
    "        \n",
    "    ################################ Gradients ################################\n",
    "\n",
    "    def compute_grads(self,verbose = False,compute_avg = True,compute_max = True):\n",
    "        \"\"\"\n",
    "        compute the gradients for the fav images of all filters of all layers for the model_info\n",
    "        Args:\n",
    "            model_info (dic): as described above\n",
    "            origin_path (str): path where to store the folders containing the gradient images\n",
    "        \"\"\"\n",
    "        self.set_state(State.compute_grad)\n",
    "        gbp = GuidedBackprop(self.model)\n",
    "        for i,lay_info in enumerate(self.conv_layinfos):\n",
    "            if (i >= self.N_LAYERS_DEV and self.DEVELOPMENT):\n",
    "                break\n",
    "            for j,filt in enumerate(lay_info['filters']):\n",
    "                clear_output(wait=True)\n",
    "                if (self.DEVELOPMENT):\n",
    "                    print(\"Grads Progression:layer{}/{} {}%\".format(i+1,self.N_LAYERS_DEV,j/self.N_FILTERS_DEV*100))\n",
    "                    if (j >=self.N_FILTERS_DEV):\n",
    "                        break\n",
    "                else:\n",
    "                    print(\"Grads Progression:layer{}/{} {}%\".format(i+1,len(self.model_info),j/len(lay_info['filters'])*100))\n",
    "                if compute_avg:\n",
    "                    path = self.__get_filt_dir(\"avg_act_grad\",lay_info['name'],filt[\"id\"])\n",
    "                    self.__compute_grads_filt(gbp,filt,path,lay_info['id'],\"avg_imgs\")\n",
    "                if compute_max:\n",
    "                    path = self.__get_filt_dir(\"max_act_grad\",lay_info['name'],filt[\"id\"])\n",
    "                    self.__compute_grads_filt(gbp,filt,path,lay_info['id'],\"max_imgs\")\n",
    "                    self.__save_cropped(grad= True)\n",
    "        self.set_state(State.idle)\n",
    "\n",
    "    def __compute_grads_filt(self,gbp,filt,path,lay_id,img_type):\n",
    "        \"\"\"\n",
    "        compute the gradients wrt to the favourite images of a filter filt.\n",
    "        Args:\n",
    "            gbp (GuidedBackprop): fitted on the model\n",
    "            filt (dic): filter from a layer\n",
    "            path (str): path to the folder where to store the gradient images\n",
    "            img_type (str): either \"avg_imgs\" or \"max_imgs\"\n",
    "        \"\"\"\n",
    "        grad_strindx = \"avg_imgs_grad\" if img_type == \"avg_imgs\" else \"max_imgs_grad\"\n",
    "\n",
    "        for i,img_path in enumerate(filt[img_type]):\n",
    "            if (img_path == self.NUMB_PATH):\n",
    "                continue   \n",
    "\n",
    "            #name of the image\n",
    "            img_name = self.__extract_name(img_path,\"_grad.jpg\")\n",
    "            #joined path and imagename\n",
    "            grad_path = os.path.join(path,img_name)\n",
    "\n",
    "            try:\n",
    "                f = open(grad_path)\n",
    "                filt[grad_strindx][i] = grad_path\n",
    "                f.close()\n",
    "                continue\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "            image = Image.open(img_path)\n",
    "            image = self.preprocess(image).unsqueeze(0)\n",
    "            image.requires_grad = True\n",
    "            class_name = img_path.split(\"/\")[-1].split(\"_\")[0]\n",
    "            gradient = gbp.generate_gradients(image,self.CLASS2LAB[class_name],lay_id,filt['id'])\n",
    "            #normalization of the gradient\n",
    "            gradient = gradient - gradient.min()\n",
    "            gradient /= gradient.max()\n",
    "            im = ToPILImage()(gradient[0])\n",
    "            im.save(grad_path)\n",
    "            filt[grad_strindx][i] = grad_path\n",
    "            \n",
    "    ################################ Plotting ################################\n",
    "\n",
    "    def plot_hist(self,layer_indx,filt_indx,hist_type=\"max\"):\n",
    "        \"\"\"\n",
    "        layer_indx(int): index of the layer\n",
    "        filt_indx(int): index of the filter\n",
    "        hist_type(str): either \"max\" or \"avg\": which hist to plot\n",
    "        \"\"\"\n",
    "        assert(hist_type == \"max\" or hist_type == \"avg\")\n",
    "        lay_info = self.model_info[layer_indx]\n",
    "        assert(isinstance(lay_info['lay'],nn.Conv2d))\n",
    "        filt = lay_info['filters'][filt_indx]\n",
    "        obj = filt['histo_counts_max'] if hist_type == \"max\" else filt['histo_counts_avg']\n",
    "        fig,ax = plt.subplots(1,1,figsize=(10,20))\n",
    "        ax.barh(list(obj.keys())[::-1],list(obj.values())[::-1])\n",
    "\n",
    "    def plot_filter_viz(self,layer_indx,filt_indx):\n",
    "        \"\"\"\n",
    "        plot the filter visualization\n",
    "        Parameters:\n",
    "        -----------\n",
    "        layer_indx : int\n",
    "            index of the layer\n",
    "        filt_indx : int\n",
    "            index of the filter\n",
    "        \"\"\"\n",
    "        filt = self.conv_layinfos[layer_indx]['filters'][filt_indx]\n",
    "        im = Image.open(filt[\"filter_viz\"])\n",
    "        plt.imshow(np.asarray(im))\n",
    "\n",
    "    def plot_top(self,kind,layer_indx,filt_indx,add_grad=False):\n",
    "        \"\"\"\n",
    "        plot the top activation samples of the train set for a given filter\n",
    "        Parameters:\n",
    "        -----------\n",
    "        kind : str\n",
    "            either \"avg\" or \"max\"\n",
    "        layer_indx : int\n",
    "            index of the layer\n",
    "        filt_indx : int\n",
    "            index of the filter\n",
    "        add_grad : bool\n",
    "            whether to plot the corresponding gradients below the original image\n",
    "        \"\"\"\n",
    "        filt = self.conv_layinfos[layer_indx]['filters'][filt_indx]\n",
    "        imgs = filt['{}_imgs'.format(kind)]\n",
    "        N = len(imgs)\n",
    "        if add_grad:\n",
    "            imgs = imgs + filt[\"{}_imgs_grad\".format(kind)]\n",
    "        M = 2 if add_grad else 1\n",
    "        fig,axes = plt.subplots(M,N,figsize=(10,10))\n",
    "        for i,(ax,img) in enumerate(zip(axes.flatten(),imgs)):\n",
    "            im_pil = Image.open(img)\n",
    "            ax.imshow(np.asarray(im_pil))\n",
    "    def plot_crop(self,layer_indx,filt_indx,grad=False):\n",
    "        \"\"\"\n",
    "        plot the top max images along with the cropped version which leads to the highest activating output for the given filter\n",
    "        Parameters:\n",
    "        -----------\n",
    "        layer_indx : int\n",
    "            index of the layer\n",
    "        filt_indx : int\n",
    "            index of the filter\n",
    "        grad : bool\n",
    "            whether to plot grad or the original image\n",
    "        \"\"\" \n",
    "        key = \"max_imgs\"\n",
    "        key_crop = \"max_imgs_crop\" \n",
    "        if grad:\n",
    "            key += \"_grad\"\n",
    "            key_crop += \"_grad\"\n",
    "        filt = self.conv_layinfos[layer_indx]['filters'][filt_indx]\n",
    "        imgs = filt[key]\n",
    "        crop_imgs = filt[key_crop]\n",
    "        fig,axes = plt.subplots(2,len(imgs),figsize=(10,10))\n",
    "        for i,(ax,img) in enumerate(zip(axes.flatten(),imgs+crop_imgs)):\n",
    "            im_pil = Image.open(img)\n",
    "            ax.imshow(np.asarray(im_pil))\n",
    "        ################################ Serving ################################\n",
    "    def serve(self,port=5000):\n",
    "        self.p = sp.Popen([sys.executable, '../app.py','--port',str(port)], \n",
    "                                    stdout=sp.PIPE, \n",
    "                                    stderr=sp.STDOUT)\n",
    "    def end_serve(self):\n",
    "        self.p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At that stage, Download the tinyimagenet dataset on [this link](https://www.kaggle.com/ifigotin/imagenetmini-1000#n01440764_10470.JPEG) and place it in the directory data (s.t the path looks like `data/tinyimagenet/rest_of_path`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we center-crop the images to the desired shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progression:2.70%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-256e6f7007f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrop_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../bigdata/imagenet-mini/train/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Cours/DataVis/com-480-project-ethiopia26/src/misc_funcs.py\u001b[0m in \u001b[0;36mcrop_imgs\u001b[0;34m(path_to_imgs_dirs)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cropped terminated successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCropped\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \"\"\"\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mcenter_crop\u001b[0;34m(img, output_size)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0mcrop_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_height\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcrop_height\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0mcrop_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_width\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcrop_width\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_top\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mcrop\u001b[0;34m(img, top, left, height, width)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img should be PIL Image. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mcrop\u001b[0;34m(self, box)\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "crop_imgs(\"../bigdata/imagenet-mini/train/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is downloaded, we need to get rid of a few buggy bw images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progression:100.00%\n",
      "BW files found:\n",
      "../data/imagenet-mini/train/n03920288/n03920288_3315.JPEG\n",
      "../data/imagenet-mini/train/n03920288/n03920288_3629.JPEG\n",
      "../data/imagenet-mini/train/n03942813/n03942813_1800.JPEG\n",
      "../data/imagenet-mini/train/n02791270/n02791270_16505.JPEG\n",
      "../data/imagenet-mini/train/n02791270/n02791270_23048.JPEG\n",
      "../data/imagenet-mini/train/n02950826/n02950826_11905.JPEG\n",
      "../data/imagenet-mini/train/n02950826/n02950826_13787.JPEG\n",
      "../data/imagenet-mini/train/n02894605/n02894605_26115.JPEG\n",
      "../data/imagenet-mini/train/n03782006/n03782006_37777.JPEG\n",
      "../data/imagenet-mini/train/n04041544/n04041544_1401.JPEG\n",
      "../data/imagenet-mini/train/n02403003/n02403003_656.JPEG\n",
      "../data/imagenet-mini/train/n02109047/n02109047_1579.JPEG\n",
      "../data/imagenet-mini/train/n02109047/n02109047_6125.JPEG\n",
      "../data/imagenet-mini/train/n02787622/n02787622_5870.JPEG\n",
      "../data/imagenet-mini/train/n02787622/n02787622_9908.JPEG\n",
      "../data/imagenet-mini/train/n02102480/n02102480_6584.JPEG\n",
      "../data/imagenet-mini/train/n03063689/n03063689_5020.JPEG\n",
      "../data/imagenet-mini/train/n03063689/n03063689_4678.JPEG\n",
      "../data/imagenet-mini/train/n03063689/n03063689_14711.JPEG\n",
      "../data/imagenet-mini/train/n03063689/n03063689_15384.JPEG\n",
      "../data/imagenet-mini/train/n02110958/n02110958_15441.JPEG\n",
      "../data/imagenet-mini/train/n02124075/n02124075_12703.JPEG\n",
      "../data/imagenet-mini/train/n03271574/n03271574_14752.JPEG\n",
      "../data/imagenet-mini/train/n03271574/n03271574_9789.JPEG\n",
      "../data/imagenet-mini/train/n04067472/n04067472_17256.JPEG\n",
      "../data/imagenet-mini/train/n04067472/n04067472_2454.JPEG\n",
      "../data/imagenet-mini/train/n04099969/n04099969_2722.JPEG\n",
      "../data/imagenet-mini/train/n04099969/n04099969_330.JPEG\n",
      "../data/imagenet-mini/train/n04099969/n04099969_10503.JPEG\n",
      "../data/imagenet-mini/train/n04099969/n04099969_7966.JPEG\n",
      "../data/imagenet-mini/train/n02097298/n02097298_12964.JPEG\n",
      "../data/imagenet-mini/train/n03532672/n03532672_38571.JPEG\n",
      "../data/imagenet-mini/train/n03532672/n03532672_48344.JPEG\n",
      "../data/imagenet-mini/train/n03532672/n03532672_28536.JPEG\n",
      "../data/imagenet-mini/train/n01440764/n01440764_15560.JPEG\n",
      "../data/imagenet-mini/train/n01704323/n01704323_1431.JPEG\n",
      "../data/imagenet-mini/train/n01704323/n01704323_7030.JPEG\n",
      "../data/imagenet-mini/train/n01704323/n01704323_8354.JPEG\n",
      "../data/imagenet-mini/train/n01704323/n01704323_1968.JPEG\n",
      "../data/imagenet-mini/train/n03141823/n03141823_3933.JPEG\n",
      "../data/imagenet-mini/train/n02443484/n02443484_1796.JPEG\n",
      "../data/imagenet-mini/train/n02096585/n02096585_10795.JPEG\n",
      "../data/imagenet-mini/train/n02096585/n02096585_6466.JPEG\n",
      "../data/imagenet-mini/train/n02096585/n02096585_10516.JPEG\n",
      "../data/imagenet-mini/train/n02974003/n02974003_9231.JPEG\n",
      "../data/imagenet-mini/train/n03347037/n03347037_17993.JPEG\n",
      "../data/imagenet-mini/train/n02236044/n02236044_4695.JPEG\n",
      "../data/imagenet-mini/train/n02113712/n02113712_4748.JPEG\n",
      "../data/imagenet-mini/train/n02500267/n02500267_7635.JPEG\n",
      "../data/imagenet-mini/train/n03874293/n03874293_12186.JPEG\n",
      "../data/imagenet-mini/train/n03133878/n03133878_3882.JPEG\n",
      "../data/imagenet-mini/train/n03133878/n03133878_1222.JPEG\n",
      "../data/imagenet-mini/train/n03786901/n03786901_10914.JPEG\n",
      "../data/imagenet-mini/train/n04040759/n04040759_38253.JPEG\n",
      "../data/imagenet-mini/train/n04040759/n04040759_12579.JPEG\n",
      "../data/imagenet-mini/train/n03250847/n03250847_13526.JPEG\n",
      "../data/imagenet-mini/train/n03250847/n03250847_16601.JPEG\n",
      "../data/imagenet-mini/train/n03498962/n03498962_9124.JPEG\n",
      "../data/imagenet-mini/train/n03759954/n03759954_19880.JPEG\n",
      "../data/imagenet-mini/train/n02676566/n02676566_9407.JPEG\n",
      "../data/imagenet-mini/train/n03697007/n03697007_171.JPEG\n",
      "../data/imagenet-mini/train/n03388183/n03388183_13836.JPEG\n",
      "../data/imagenet-mini/train/n03950228/n03950228_11028.JPEG\n",
      "../data/imagenet-mini/train/n03950228/n03950228_11161.JPEG\n",
      "../data/imagenet-mini/train/n15075141/n15075141_10244.JPEG\n",
      "../data/imagenet-mini/train/n15075141/n15075141_3267.JPEG\n",
      "../data/imagenet-mini/train/n04366367/n04366367_3874.JPEG\n",
      "../data/imagenet-mini/train/n04366367/n04366367_11770.JPEG\n",
      "../data/imagenet-mini/train/n04366367/n04366367_34620.JPEG\n",
      "../data/imagenet-mini/train/n03832673/n03832673_220.JPEG\n",
      "../data/imagenet-mini/train/n03832673/n03832673_19442.JPEG\n",
      "../data/imagenet-mini/train/n03832673/n03832673_5334.JPEG\n",
      "../data/imagenet-mini/train/n03642806/n03642806_9700.JPEG\n",
      "../data/imagenet-mini/train/n03642806/n03642806_3515.JPEG\n",
      "../data/imagenet-mini/train/n02814860/n02814860_17864.JPEG\n",
      "../data/imagenet-mini/train/n02988304/n02988304_8210.JPEG\n",
      "../data/imagenet-mini/train/n02112137/n02112137_279.JPEG\n",
      "../data/imagenet-mini/train/n03887697/n03887697_4952.JPEG\n",
      "../data/imagenet-mini/train/n02965783/n02965783_4887.JPEG\n",
      "../data/imagenet-mini/train/n02823428/n02823428_3803.JPEG\n",
      "../data/imagenet-mini/train/n03255030/n03255030_11039.JPEG\n",
      "../data/imagenet-mini/train/n03255030/n03255030_8885.JPEG\n",
      "../data/imagenet-mini/train/n03255030/n03255030_9247.JPEG\n",
      "../data/imagenet-mini/train/n03255030/n03255030_11118.JPEG\n",
      "../data/imagenet-mini/train/n03933933/n03933933_6303.JPEG\n",
      "../data/imagenet-mini/train/n03933933/n03933933_24509.JPEG\n",
      "../data/imagenet-mini/train/n04428191/n04428191_43456.JPEG\n",
      "../data/imagenet-mini/train/n04428191/n04428191_2240.JPEG\n",
      "../data/imagenet-mini/train/n04428191/n04428191_6483.JPEG\n",
      "../data/imagenet-mini/train/n02091244/n02091244_8124.JPEG\n",
      "../data/imagenet-mini/train/n02808440/n02808440_55396.JPEG\n",
      "../data/imagenet-mini/train/n02808440/n02808440_47134.JPEG\n",
      "../data/imagenet-mini/train/n02966687/n02966687_3742.JPEG\n",
      "../data/imagenet-mini/train/n02091831/n02091831_3598.JPEG\n",
      "../data/imagenet-mini/train/n03976467/n03976467_3522.JPEG\n",
      "../data/imagenet-mini/train/n03976467/n03976467_16598.JPEG\n",
      "../data/imagenet-mini/train/n03976467/n03976467_20470.JPEG\n",
      "../data/imagenet-mini/train/n04347754/n04347754_32095.JPEG\n",
      "../data/imagenet-mini/train/n04347754/n04347754_3334.JPEG\n",
      "../data/imagenet-mini/train/n03404251/n03404251_9354.JPEG\n",
      "../data/imagenet-mini/train/n03733281/n03733281_23036.JPEG\n",
      "../data/imagenet-mini/train/n04074963/n04074963_19650.JPEG\n",
      "../data/imagenet-mini/train/n04074963/n04074963_6499.JPEG\n",
      "../data/imagenet-mini/train/n03916031/n03916031_41551.JPEG\n",
      "../data/imagenet-mini/train/n04392985/n04392985_2489.JPEG\n",
      "../data/imagenet-mini/train/n04392985/n04392985_4047.JPEG\n",
      "../data/imagenet-mini/train/n03998194/n03998194_6751.JPEG\n",
      "../data/imagenet-mini/train/n04515003/n04515003_32327.JPEG\n",
      "../data/imagenet-mini/train/n04515003/n04515003_10380.JPEG\n",
      "../data/imagenet-mini/train/n02071294/n02071294_6873.JPEG\n",
      "../data/imagenet-mini/train/n03794056/n03794056_13949.JPEG\n",
      "../data/imagenet-mini/train/n03794056/n03794056_10172.JPEG\n",
      "../data/imagenet-mini/train/n03794056/n03794056_10490.JPEG\n",
      "../data/imagenet-mini/train/n04507155/n04507155_14941.JPEG\n",
      "../data/imagenet-mini/train/n04090263/n04090263_15504.JPEG\n",
      "../data/imagenet-mini/train/n02002556/n02002556_5205.JPEG\n",
      "../data/imagenet-mini/train/n02088238/n02088238_1485.JPEG\n",
      "../data/imagenet-mini/train/n02092002/n02092002_12862.JPEG\n",
      "../data/imagenet-mini/train/n02883205/n02883205_60864.JPEG\n",
      "../data/imagenet-mini/train/n02883205/n02883205_2488.JPEG\n",
      "../data/imagenet-mini/train/n02883205/n02883205_16367.JPEG\n",
      "../data/imagenet-mini/train/n02883205/n02883205_16443.JPEG\n",
      "../data/imagenet-mini/train/n02883205/n02883205_23760.JPEG\n",
      "../data/imagenet-mini/train/n02795169/n02795169_11492.JPEG\n",
      "../data/imagenet-mini/train/n02795169/n02795169_7453.JPEG\n",
      "../data/imagenet-mini/train/n03773504/n03773504_23078.JPEG\n",
      "../data/imagenet-mini/train/n03773504/n03773504_25680.JPEG\n",
      "../data/imagenet-mini/train/n04482393/n04482393_3891.JPEG\n",
      "../data/imagenet-mini/train/n02930766/n02930766_20313.JPEG\n",
      "../data/imagenet-mini/train/n04296562/n04296562_18979.JPEG\n",
      "../data/imagenet-mini/train/n02108915/n02108915_6894.JPEG\n",
      "../data/imagenet-mini/train/n03085013/n03085013_5430.JPEG\n",
      "../data/imagenet-mini/train/n03085013/n03085013_20901.JPEG\n",
      "../data/imagenet-mini/train/n03085013/n03085013_22945.JPEG\n",
      "../data/imagenet-mini/train/n03085013/n03085013_12140.JPEG\n",
      "../data/imagenet-mini/train/n03085013/n03085013_21506.JPEG\n",
      "../data/imagenet-mini/train/n03085013/n03085013_23775.JPEG\n",
      "../data/imagenet-mini/train/n04591157/n04591157_576.JPEG\n",
      "../data/imagenet-mini/train/n02328150/n02328150_1797.JPEG\n",
      "../data/imagenet-mini/train/n02791124/n02791124_1928.JPEG\n",
      "../data/imagenet-mini/train/n02791124/n02791124_6387.JPEG\n",
      "../data/imagenet-mini/train/n04238763/n04238763_22437.JPEG\n",
      "../data/imagenet-mini/train/n03763968/n03763968_6975.JPEG\n",
      "../data/imagenet-mini/train/n03062245/n03062245_4527.JPEG\n",
      "../data/imagenet-mini/train/n04141076/n04141076_43949.JPEG\n",
      "../data/imagenet-mini/train/n04141076/n04141076_75558.JPEG\n",
      "../data/imagenet-mini/train/n04141076/n04141076_71154.JPEG\n",
      "../data/imagenet-mini/train/n04141076/n04141076_73894.JPEG\n",
      "../data/imagenet-mini/train/n04447861/n04447861_3339.JPEG\n",
      "../data/imagenet-mini/train/n03376595/n03376595_6525.JPEG\n",
      "../data/imagenet-mini/train/n04023962/n04023962_9274.JPEG\n",
      "../data/imagenet-mini/train/n04023962/n04023962_37636.JPEG\n",
      "../data/imagenet-mini/train/n04023962/n04023962_50943.JPEG\n",
      "../data/imagenet-mini/train/n03793489/n03793489_10431.JPEG\n",
      "../data/imagenet-mini/train/n04070727/n04070727_39231.JPEG\n",
      "../data/imagenet-mini/train/n04070727/n04070727_60011.JPEG\n",
      "../data/imagenet-mini/train/n02120505/n02120505_15958.JPEG\n",
      "../data/imagenet-mini/train/n02483362/n02483362_6436.JPEG\n",
      "../data/imagenet-mini/train/n04442312/n04442312_14092.JPEG\n",
      "../data/imagenet-mini/train/n04442312/n04442312_11526.JPEG\n",
      "../data/imagenet-mini/train/n03065424/n03065424_17272.JPEG\n",
      "../data/imagenet-mini/train/n03065424/n03065424_68951.JPEG\n",
      "../data/imagenet-mini/train/n03924679/n03924679_10873.JPEG\n",
      "../data/imagenet-mini/train/n02892767/n02892767_4201.JPEG\n",
      "../data/imagenet-mini/train/n02107312/n02107312_2048.JPEG\n",
      "../data/imagenet-mini/train/n04590129/n04590129_22609.JPEG\n",
      "../data/imagenet-mini/train/n04590129/n04590129_12864.JPEG\n",
      "../data/imagenet-mini/train/n04590129/n04590129_8578.JPEG\n",
      "../data/imagenet-mini/train/n04590129/n04590129_37963.JPEG\n",
      "../data/imagenet-mini/train/n07753592/n07753592_9565.JPEG\n",
      "../data/imagenet-mini/train/n02107142/n02107142_52349.JPEG\n",
      "../data/imagenet-mini/train/n02110627/n02110627_22516.JPEG\n",
      "../data/imagenet-mini/train/n02088466/n02088466_12216.JPEG\n",
      "../data/imagenet-mini/train/n02088466/n02088466_11132.JPEG\n",
      "../data/imagenet-mini/train/n03721384/n03721384_1709.JPEG\n",
      "../data/imagenet-mini/train/n03220513/n03220513_4782.JPEG\n",
      "../data/imagenet-mini/train/n02837789/n02837789_21447.JPEG\n",
      "../data/imagenet-mini/train/n02837789/n02837789_23422.JPEG\n",
      "../data/imagenet-mini/train/n02111500/n02111500_7146.JPEG\n",
      "../data/imagenet-mini/train/n02793495/n02793495_9539.JPEG\n",
      "../data/imagenet-mini/train/n02793495/n02793495_4609.JPEG\n",
      "../data/imagenet-mini/train/n03891251/n03891251_2898.JPEG\n",
      "../data/imagenet-mini/train/n03891251/n03891251_3947.JPEG\n",
      "../data/imagenet-mini/train/n03891251/n03891251_5637.JPEG\n",
      "../data/imagenet-mini/train/n03891251/n03891251_3383.JPEG\n",
      "../data/imagenet-mini/train/n04286575/n04286575_4871.JPEG\n",
      "../data/imagenet-mini/train/n02992529/n02992529_21092.JPEG\n",
      "../data/imagenet-mini/train/n02992529/n02992529_60654.JPEG\n",
      "../data/imagenet-mini/train/n02992529/n02992529_3308.JPEG\n",
      "../data/imagenet-mini/train/n04486054/n04486054_8908.JPEG\n",
      "../data/imagenet-mini/train/n03372029/n03372029_44161.JPEG\n",
      "../data/imagenet-mini/train/n03372029/n03372029_41945.JPEG\n",
      "../data/imagenet-mini/train/n04037443/n04037443_8697.JPEG\n",
      "../data/imagenet-mini/train/n04037443/n04037443_22059.JPEG\n",
      "../data/imagenet-mini/train/n03838899/n03838899_16491.JPEG\n",
      "../data/imagenet-mini/train/n03838899/n03838899_31104.JPEG\n",
      "../data/imagenet-mini/train/n04606251/n04606251_32516.JPEG\n",
      "../data/imagenet-mini/train/n03457902/n03457902_6713.JPEG\n",
      "../data/imagenet-mini/train/n07930864/n07930864_10538.JPEG\n",
      "../data/imagenet-mini/train/n07930864/n07930864_30821.JPEG\n",
      "../data/imagenet-mini/train/n03954731/n03954731_15587.JPEG\n",
      "../data/imagenet-mini/train/n01930112/n01930112_9652.JPEG\n",
      "../data/imagenet-mini/train/n01930112/n01930112_4440.JPEG\n",
      "../data/imagenet-mini/train/n01930112/n01930112_13988.JPEG\n",
      "../data/imagenet-mini/train/n01930112/n01930112_7379.JPEG\n",
      "../data/imagenet-mini/train/n01930112/n01930112_14916.JPEG\n",
      "../data/imagenet-mini/train/n04208210/n04208210_28386.JPEG\n",
      "../data/imagenet-mini/train/n04258138/n04258138_5172.JPEG\n",
      "../data/imagenet-mini/train/n04332243/n04332243_31816.JPEG\n",
      "../data/imagenet-mini/train/n02105056/n02105056_3683.JPEG\n",
      "../data/imagenet-mini/train/n02105056/n02105056_11069.JPEG\n",
      "../data/imagenet-mini/train/n02105056/n02105056_289.JPEG\n",
      "../data/imagenet-mini/train/n02692877/n02692877_32831.JPEG\n",
      "../data/imagenet-mini/train/n02692877/n02692877_18298.JPEG\n",
      "../data/imagenet-mini/train/n02692877/n02692877_5920.JPEG\n",
      "../data/imagenet-mini/train/n02692877/n02692877_5858.JPEG\n",
      "../data/imagenet-mini/train/n02692877/n02692877_6627.JPEG\n",
      "../data/imagenet-mini/train/n02692877/n02692877_6368.JPEG\n",
      "../data/imagenet-mini/train/n02692877/n02692877_3502.JPEG\n",
      "../data/imagenet-mini/train/n02692877/n02692877_3725.JPEG\n",
      "../data/imagenet-mini/train/n02692877/n02692877_25356.JPEG\n",
      "../data/imagenet-mini/train/n02093754/n02093754_6541.JPEG\n",
      "../data/imagenet-mini/train/n01491361/n01491361_5388.JPEG\n",
      "../data/imagenet-mini/train/n01491361/n01491361_4245.JPEG\n",
      "../data/imagenet-mini/train/n03538406/n03538406_3638.JPEG\n",
      "../data/imagenet-mini/train/n04467665/n04467665_7988.JPEG\n",
      "../data/imagenet-mini/train/n04553703/n04553703_18609.JPEG\n",
      "../data/imagenet-mini/train/n04553703/n04553703_34716.JPEG\n",
      "../data/imagenet-mini/train/n01795545/n01795545_13257.JPEG\n",
      "../data/imagenet-mini/train/n04612504/n04612504_364.JPEG\n",
      "../data/imagenet-mini/train/n03595614/n03595614_7821.JPEG\n",
      "../data/imagenet-mini/train/n03657121/n03657121_18041.JPEG\n",
      "../data/imagenet-mini/train/n03657121/n03657121_470.JPEG\n",
      "../data/imagenet-mini/train/n03657121/n03657121_7288.JPEG\n",
      "../data/imagenet-mini/train/n03657121/n03657121_41088.JPEG\n",
      "../data/imagenet-mini/train/n04081281/n04081281_4577.JPEG\n",
      "../data/imagenet-mini/train/n02095889/n02095889_9201.JPEG\n",
      "../data/imagenet-mini/train/n02095889/n02095889_5132.JPEG\n",
      "../data/imagenet-mini/train/n03895866/n03895866_127963.JPEG\n",
      "../data/imagenet-mini/train/n03895866/n03895866_94175.JPEG\n",
      "../data/imagenet-mini/train/n03895866/n03895866_137050.JPEG\n",
      "../data/imagenet-mini/train/n03110669/n03110669_162283.JPEG\n",
      "../data/imagenet-mini/train/n03110669/n03110669_85296.JPEG\n",
      "../data/imagenet-mini/train/n03110669/n03110669_72318.JPEG\n",
      "../data/imagenet-mini/train/n03110669/n03110669_112169.JPEG\n",
      "../data/imagenet-mini/train/n04153751/n04153751_3405.JPEG\n",
      "../data/imagenet-mini/train/n04153751/n04153751_10444.JPEG\n",
      "../data/imagenet-mini/train/n02669723/n02669723_2185.JPEG\n",
      "../data/imagenet-mini/train/n09428293/n09428293_46398.JPEG\n",
      "../data/imagenet-mini/train/n04532670/n04532670_15008.JPEG\n",
      "../data/imagenet-mini/train/n04532670/n04532670_10007.JPEG\n",
      "../data/imagenet-mini/train/n04532670/n04532670_32887.JPEG\n",
      "../data/imagenet-mini/train/n04532670/n04532670_18431.JPEG\n",
      "../data/imagenet-mini/train/n03908714/n03908714_105.JPEG\n",
      "../data/imagenet-mini/train/n03908714/n03908714_4058.JPEG\n",
      "../data/imagenet-mini/train/n03908714/n03908714_1861.JPEG\n",
      "../data/imagenet-mini/train/n03908714/n03908714_3926.JPEG\n",
      "../data/imagenet-mini/train/n03908714/n03908714_637.JPEG\n",
      "../data/imagenet-mini/train/n03443371/n03443371_11660.JPEG\n",
      "../data/imagenet-mini/train/n03961711/n03961711_1738.JPEG\n",
      "../data/imagenet-mini/train/n03891332/n03891332_483.JPEG\n",
      "../data/imagenet-mini/train/n03891332/n03891332_4026.JPEG\n",
      "../data/imagenet-mini/train/n03673027/n03673027_22831.JPEG\n",
      "../data/imagenet-mini/train/n02951585/n02951585_17058.JPEG\n",
      "../data/imagenet-mini/train/n02951585/n02951585_91.JPEG\n",
      "../data/imagenet-mini/train/n02110341/n02110341_14123.JPEG\n",
      "../data/imagenet-mini/train/n04330267/n04330267_16564.JPEG\n",
      "../data/imagenet-mini/train/n04330267/n04330267_18079.JPEG\n",
      "../data/imagenet-mini/train/n02092339/n02092339_6110.JPEG\n",
      "../data/imagenet-mini/train/n02092339/n02092339_5116.JPEG\n",
      "../data/imagenet-mini/train/n02092339/n02092339_6429.JPEG\n",
      "../data/imagenet-mini/train/n03476991/n03476991_29558.JPEG\n",
      "../data/imagenet-mini/train/n03476991/n03476991_27682.JPEG\n",
      "../data/imagenet-mini/train/n03452741/n03452741_11213.JPEG\n",
      "../data/imagenet-mini/train/n04204238/n04204238_7491.JPEG\n",
      "../data/imagenet-mini/train/n03874599/n03874599_2951.JPEG\n",
      "../data/imagenet-mini/train/n02097047/n02097047_2545.JPEG\n",
      "../data/imagenet-mini/train/n03180011/n03180011_10259.JPEG\n",
      "../data/imagenet-mini/train/n04465501/n04465501_1377.JPEG\n",
      "../data/imagenet-mini/train/n04465501/n04465501_3331.JPEG\n",
      "../data/imagenet-mini/train/n03884397/n03884397_8436.JPEG\n",
      "../data/imagenet-mini/train/n03884397/n03884397_5419.JPEG\n",
      "../data/imagenet-mini/train/n03630383/n03630383_3798.JPEG\n",
      "../data/imagenet-mini/train/n02093647/n02093647_8044.JPEG\n",
      "../data/imagenet-mini/train/n03394916/n03394916_7045.JPEG\n",
      "../data/imagenet-mini/train/n04589890/n04589890_2205.JPEG\n",
      "../data/imagenet-mini/train/n04589890/n04589890_9771.JPEG\n",
      "../data/imagenet-mini/train/n03124043/n03124043_2249.JPEG\n",
      "../data/imagenet-mini/train/n03017168/n03017168_18051.JPEG\n",
      "../data/imagenet-mini/train/n03017168/n03017168_38025.JPEG\n",
      "../data/imagenet-mini/train/n03017168/n03017168_33511.JPEG\n",
      "../data/imagenet-mini/train/n02408429/n02408429_11593.JPEG\n",
      "../data/imagenet-mini/train/n02859443/n02859443_18078.JPEG\n",
      "../data/imagenet-mini/train/n02085620/n02085620_4629.JPEG\n",
      "../data/imagenet-mini/train/n02085620/n02085620_4210.JPEG\n",
      "../data/imagenet-mini/train/n02085620/n02085620_1194.JPEG\n",
      "../data/imagenet-mini/train/n02112706/n02112706_2856.JPEG\n",
      "../data/imagenet-mini/train/n04370456/n04370456_11533.JPEG\n",
      "../data/imagenet-mini/train/n02979186/n02979186_1587.JPEG\n",
      "../data/imagenet-mini/train/n03445777/n03445777_8971.JPEG\n",
      "../data/imagenet-mini/train/n03445777/n03445777_5325.JPEG\n",
      "../data/imagenet-mini/train/n01990800/n01990800_16602.JPEG\n",
      "../data/imagenet-mini/train/n04548280/n04548280_5424.JPEG\n",
      "../data/imagenet-mini/train/n02128925/n02128925_3789.JPEG\n",
      "../data/imagenet-mini/train/n03958227/n03958227_4724.JPEG\n",
      "../data/imagenet-mini/train/n03109150/n03109150_5636.JPEG\n",
      "../data/imagenet-mini/train/n03109150/n03109150_25112.JPEG\n",
      "../data/imagenet-mini/train/n07802026/n07802026_152.JPEG\n",
      "../data/imagenet-mini/train/n07802026/n07802026_20020.JPEG\n",
      "../data/imagenet-mini/train/n07802026/n07802026_17453.JPEG\n",
      "../data/imagenet-mini/train/n04005630/n04005630_95489.JPEG\n",
      "../data/imagenet-mini/train/n03804744/n03804744_10763.JPEG\n",
      "../data/imagenet-mini/train/n02089078/n02089078_2638.JPEG\n",
      "../data/imagenet-mini/train/n02481823/n02481823_14519.JPEG\n",
      "../data/imagenet-mini/train/n02481823/n02481823_3990.JPEG\n",
      "../data/imagenet-mini/train/n02481823/n02481823_1191.JPEG\n",
      "../data/imagenet-mini/train/n02481823/n02481823_17637.JPEG\n",
      "../data/imagenet-mini/train/n04259630/n04259630_11609.JPEG\n",
      "../data/imagenet-mini/train/n02794156/n02794156_19363.JPEG\n",
      "../data/imagenet-mini/train/n02794156/n02794156_10156.JPEG\n",
      "../data/imagenet-mini/train/n02794156/n02794156_1916.JPEG\n",
      "../data/imagenet-mini/train/n04147183/n04147183_13979.JPEG\n",
      "../data/imagenet-mini/train/n02916936/n02916936_9535.JPEG\n",
      "../data/imagenet-mini/train/n02916936/n02916936_1445.JPEG\n",
      "../data/imagenet-mini/train/n03868863/n03868863_4734.JPEG\n",
      "../data/imagenet-mini/train/n03868863/n03868863_132.JPEG\n",
      "../data/imagenet-mini/train/n03868863/n03868863_6703.JPEG\n",
      "../data/imagenet-mini/train/n04204347/n04204347_7406.JPEG\n",
      "../data/imagenet-mini/train/n01514859/n01514859_9725.JPEG\n",
      "../data/imagenet-mini/train/n04443257/n04443257_41924.JPEG\n",
      "../data/imagenet-mini/train/n04443257/n04443257_20359.JPEG\n",
      "../data/imagenet-mini/train/n04009552/n04009552_2159.JPEG\n",
      "../data/imagenet-mini/train/n03873416/n03873416_4377.JPEG\n",
      "../data/imagenet-mini/train/n03272562/n03272562_7870.JPEG\n",
      "../data/imagenet-mini/train/n03127925/n03127925_3988.JPEG\n",
      "../data/imagenet-mini/train/n02105641/n02105641_3786.JPEG\n",
      "../data/imagenet-mini/train/n02111889/n02111889_3691.JPEG\n",
      "../data/imagenet-mini/train/n02708093/n02708093_304.JPEG\n",
      "../data/imagenet-mini/train/n02708093/n02708093_3206.JPEG\n",
      "../data/imagenet-mini/train/n04254680/n04254680_7339.JPEG\n",
      "../data/imagenet-mini/train/n03743016/n03743016_14769.JPEG\n",
      "../data/imagenet-mini/train/n03743016/n03743016_4320.JPEG\n",
      "../data/imagenet-mini/train/n04562935/n04562935_6426.JPEG\n",
      "../data/imagenet-mini/train/n04118776/n04118776_68075.JPEG\n",
      "../data/imagenet-mini/train/n04118776/n04118776_3546.JPEG\n",
      "../data/imagenet-mini/train/n02091134/n02091134_1526.JPEG\n",
      "../data/imagenet-mini/train/n04458633/n04458633_3571.JPEG\n",
      "../data/imagenet-mini/train/n04458633/n04458633_4746.JPEG\n",
      "../data/imagenet-mini/train/n03344393/n03344393_6191.JPEG\n",
      "../data/imagenet-mini/train/n02999410/n02999410_10081.JPEG\n",
      "../data/imagenet-mini/train/n02999410/n02999410_19414.JPEG\n",
      "../data/imagenet-mini/train/n02999410/n02999410_8597.JPEG\n",
      "../data/imagenet-mini/train/n02999410/n02999410_17559.JPEG\n",
      "../data/imagenet-mini/train/n02105505/n02105505_2467.JPEG\n",
      "../data/imagenet-mini/train/n02105505/n02105505_5898.JPEG\n",
      "../data/imagenet-mini/train/n04525305/n04525305_4072.JPEG\n",
      "../data/imagenet-mini/train/n03467068/n03467068_8691.JPEG\n",
      "../data/imagenet-mini/train/n03467068/n03467068_10035.JPEG\n",
      "../data/imagenet-mini/train/n03467068/n03467068_8800.JPEG\n",
      "../data/imagenet-mini/train/n04008634/n04008634_25354.JPEG\n",
      "../data/imagenet-mini/train/n04008634/n04008634_18231.JPEG\n",
      "../data/imagenet-mini/train/n04069434/n04069434_21934.JPEG\n",
      "../data/imagenet-mini/train/n04069434/n04069434_4001.JPEG\n",
      "../data/imagenet-mini/train/n04069434/n04069434_11503.JPEG\n",
      "../data/imagenet-mini/train/n02488291/n02488291_1558.JPEG\n",
      "../data/imagenet-mini/train/n02488291/n02488291_1510.JPEG\n",
      "../data/imagenet-mini/train/n04263257/n04263257_1946.JPEG\n",
      "../data/imagenet-mini/train/n03888605/n03888605_36989.JPEG\n",
      "../data/imagenet-mini/train/n04505470/n04505470_5154.JPEG\n",
      "../data/imagenet-mini/train/n04505470/n04505470_2769.JPEG\n",
      "../data/imagenet-mini/train/n03992509/n03992509_1708.JPEG\n",
      "../data/imagenet-mini/train/n03992509/n03992509_14593.JPEG\n",
      "../data/imagenet-mini/train/n03992509/n03992509_8228.JPEG\n",
      "../data/imagenet-mini/train/n03992509/n03992509_874.JPEG\n",
      "../data/imagenet-mini/train/n03658185/n03658185_4260.JPEG\n",
      "../data/imagenet-mini/train/n02860847/n02860847_6819.JPEG\n",
      "../data/imagenet-mini/train/n02860847/n02860847_29748.JPEG\n",
      "../data/imagenet-mini/train/n02099267/n02099267_512.JPEG\n",
      "../data/imagenet-mini/train/n03982430/n03982430_27117.JPEG\n",
      "../data/imagenet-mini/train/n04317175/n04317175_6652.JPEG\n",
      "../data/imagenet-mini/train/n04536866/n04536866_3282.JPEG\n",
      "../data/imagenet-mini/train/n04536866/n04536866_20544.JPEG\n",
      "../data/imagenet-mini/train/n04536866/n04536866_3629.JPEG\n",
      "../data/imagenet-mini/train/n04536866/n04536866_12210.JPEG\n",
      "../data/imagenet-mini/train/n04536866/n04536866_12630.JPEG\n",
      "../data/imagenet-mini/train/n02749479/n02749479_89.JPEG\n",
      "../data/imagenet-mini/train/n02749479/n02749479_29157.JPEG\n",
      "../data/imagenet-mini/train/n02749479/n02749479_4779.JPEG\n",
      "../data/imagenet-mini/train/n02749479/n02749479_932.JPEG\n",
      "../data/imagenet-mini/train/n03075370/n03075370_2458.JPEG\n",
      "../data/imagenet-mini/train/n03075370/n03075370_9413.JPEG\n",
      "../data/imagenet-mini/train/n03483316/n03483316_11347.JPEG\n",
      "../data/imagenet-mini/train/n03483316/n03483316_21775.JPEG\n",
      "../data/imagenet-mini/train/n04131690/n04131690_8589.JPEG\n",
      "../data/imagenet-mini/train/n02879718/n02879718_30188.JPEG\n",
      "../data/imagenet-mini/train/n03000134/n03000134_4747.JPEG\n",
      "../data/imagenet-mini/train/n03000134/n03000134_2546.JPEG\n",
      "../data/imagenet-mini/train/n03000134/n03000134_1753.JPEG\n",
      "../data/imagenet-mini/train/n03000134/n03000134_14.JPEG\n",
      "../data/imagenet-mini/train/n02895154/n02895154_9919.JPEG\n",
      "../data/imagenet-mini/train/n03902125/n03902125_21133.JPEG\n",
      "../data/imagenet-mini/train/n02093859/n02093859_4345.JPEG\n",
      "../data/imagenet-mini/train/n04200800/n04200800_38142.JPEG\n",
      "../data/imagenet-mini/train/n03661043/n03661043_7787.JPEG\n",
      "../data/imagenet-mini/train/n04209239/n04209239_11024.JPEG\n",
      "../data/imagenet-mini/train/n04209239/n04209239_8143.JPEG\n",
      "../data/imagenet-mini/train/n03424325/n03424325_23753.JPEG\n",
      "../data/imagenet-mini/train/n03424325/n03424325_5687.JPEG\n",
      "../data/imagenet-mini/train/n03602883/n03602883_11170.JPEG\n",
      "../data/imagenet-mini/train/n02091467/n02091467_4823.JPEG\n",
      "../data/imagenet-mini/train/n02091467/n02091467_12614.JPEG\n",
      "../data/imagenet-mini/train/n04127249/n04127249_2853.JPEG\n",
      "../data/imagenet-mini/train/n03495258/n03495258_9193.JPEG\n",
      "../data/imagenet-mini/train/n03495258/n03495258_16828.JPEG\n",
      "../data/imagenet-mini/train/n03495258/n03495258_25470.JPEG\n",
      "../data/imagenet-mini/train/n03970156/n03970156_28533.JPEG\n",
      "../data/imagenet-mini/train/n02391049/n02391049_1508.JPEG\n",
      "../data/imagenet-mini/train/n03930313/n03930313_474.JPEG\n",
      "../data/imagenet-mini/train/n02085936/n02085936_14974.JPEG\n",
      "../data/imagenet-mini/train/n03777754/n03777754_2785.JPEG\n",
      "../data/imagenet-mini/train/n02978881/n02978881_607.JPEG\n",
      "../data/imagenet-mini/train/n02978881/n02978881_43638.JPEG\n",
      "../data/imagenet-mini/train/n02978881/n02978881_12420.JPEG\n",
      "../data/imagenet-mini/train/n02978881/n02978881_12824.JPEG\n",
      "../data/imagenet-mini/train/n01494475/n01494475_4148.JPEG\n",
      "../data/imagenet-mini/train/n03825788/n03825788_6706.JPEG\n",
      "../data/imagenet-mini/train/n03494278/n03494278_38369.JPEG\n",
      "../data/imagenet-mini/train/n03494278/n03494278_34652.JPEG\n",
      "../data/imagenet-mini/train/n03633091/n03633091_12244.JPEG\n",
      "../data/imagenet-mini/train/n04509417/n04509417_4949.JPEG\n",
      "../data/imagenet-mini/train/n04509417/n04509417_6017.JPEG\n",
      "../data/imagenet-mini/train/n07920052/n07920052_599.JPEG\n",
      "../data/imagenet-mini/train/n01737021/n01737021_429.JPEG\n",
      "../data/imagenet-mini/train/n03599486/n03599486_13087.JPEG\n",
      "../data/imagenet-mini/train/n04554684/n04554684_4076.JPEG\n",
      "../data/imagenet-mini/train/n02939185/n02939185_21836.JPEG\n",
      "../data/imagenet-mini/train/n02104029/n02104029_7622.JPEG\n",
      "../data/imagenet-mini/train/n04328186/n04328186_48308.JPEG\n",
      "../data/imagenet-mini/train/n04328186/n04328186_4766.JPEG\n",
      "../data/imagenet-mini/train/n04328186/n04328186_8361.JPEG\n",
      "../data/imagenet-mini/train/n03692522/n03692522_12000.JPEG\n",
      "../data/imagenet-mini/train/n03692522/n03692522_4410.JPEG\n",
      "../data/imagenet-mini/train/n03692522/n03692522_11536.JPEG\n",
      "../data/imagenet-mini/train/n04019541/n04019541_8604.JPEG\n",
      "../data/imagenet-mini/train/n04356056/n04356056_60202.JPEG\n",
      "../data/imagenet-mini/train/n04356056/n04356056_2540.JPEG\n",
      "../data/imagenet-mini/train/n06785654/n06785654_3588.JPEG\n",
      "../data/imagenet-mini/train/n06785654/n06785654_11253.JPEG\n",
      "../data/imagenet-mini/train/n06785654/n06785654_12009.JPEG\n",
      "../data/imagenet-mini/train/n06785654/n06785654_3680.JPEG\n",
      "../data/imagenet-mini/train/n06785654/n06785654_3346.JPEG\n",
      "../data/imagenet-mini/train/n06785654/n06785654_2319.JPEG\n",
      "../data/imagenet-mini/train/n04336792/n04336792_19464.JPEG\n",
      "../data/imagenet-mini/train/n04336792/n04336792_42773.JPEG\n",
      "../data/imagenet-mini/train/n04336792/n04336792_18677.JPEG\n",
      "../data/imagenet-mini/train/n04162706/n04162706_46463.JPEG\n",
      "../data/imagenet-mini/train/n04162706/n04162706_18311.JPEG\n",
      "../data/imagenet-mini/train/n04162706/n04162706_10402.JPEG\n",
      "../data/imagenet-mini/train/n03903868/n03903868_50629.JPEG\n",
      "../data/imagenet-mini/train/n04141975/n04141975_35800.JPEG\n",
      "../data/imagenet-mini/train/n04141975/n04141975_15.JPEG\n",
      "../data/imagenet-mini/train/n04141975/n04141975_11858.JPEG\n",
      "../data/imagenet-mini/train/n04141975/n04141975_34852.JPEG\n",
      "../data/imagenet-mini/train/n04154565/n04154565_25237.JPEG\n",
      "../data/imagenet-mini/train/n04423845/n04423845_3482.JPEG\n",
      "../data/imagenet-mini/train/n04423845/n04423845_10470.JPEG\n",
      "../data/imagenet-mini/train/n04487081/n04487081_11484.JPEG\n",
      "../data/imagenet-mini/train/n04560804/n04560804_10316.JPEG\n",
      "../data/imagenet-mini/train/n04560804/n04560804_13629.JPEG\n",
      "../data/imagenet-mini/train/n04560804/n04560804_4935.JPEG\n",
      "../data/imagenet-mini/train/n04039381/n04039381_19558.JPEG\n",
      "../data/imagenet-mini/train/n02113624/n02113624_6418.JPEG\n",
      "../data/imagenet-mini/train/n02687172/n02687172_34236.JPEG\n",
      "../data/imagenet-mini/train/n02992211/n02992211_26518.JPEG\n",
      "../data/imagenet-mini/train/n02992211/n02992211_37588.JPEG\n",
      "../data/imagenet-mini/train/n02992211/n02992211_30118.JPEG\n",
      "../data/imagenet-mini/train/n02992211/n02992211_33045.JPEG\n",
      "../data/imagenet-mini/train/n02992211/n02992211_34104.JPEG\n",
      "../data/imagenet-mini/train/n02992211/n02992211_33025.JPEG\n",
      "../data/imagenet-mini/train/n02992211/n02992211_37010.JPEG\n",
      "../data/imagenet-mini/train/n03929660/n03929660_969.JPEG\n",
      "../data/imagenet-mini/train/n03929660/n03929660_20053.JPEG\n",
      "../data/imagenet-mini/train/n03929660/n03929660_2693.JPEG\n",
      "../data/imagenet-mini/train/n03929660/n03929660_56931.JPEG\n",
      "../data/imagenet-mini/train/n03929660/n03929660_13396.JPEG\n",
      "../data/imagenet-mini/train/n03297495/n03297495_628.JPEG\n",
      "../data/imagenet-mini/train/n04417672/n04417672_13599.JPEG\n",
      "../data/imagenet-mini/train/n03777568/n03777568_1037.JPEG\n",
      "../data/imagenet-mini/train/n03777568/n03777568_17270.JPEG\n",
      "../data/imagenet-mini/train/n04461696/n04461696_2394.JPEG\n",
      "../data/imagenet-mini/train/n04326547/n04326547_12133.JPEG\n",
      "../data/imagenet-mini/train/n02013706/n02013706_4625.JPEG\n",
      "../data/imagenet-mini/train/n04501370/n04501370_11469.JPEG\n",
      "../data/imagenet-mini/train/n03929855/n03929855_6199.JPEG\n",
      "../data/imagenet-mini/train/n02086240/n02086240_3799.JPEG\n",
      "../data/imagenet-mini/train/n02480855/n02480855_252.JPEG\n",
      "../data/imagenet-mini/train/n02480855/n02480855_17876.JPEG\n",
      "../data/imagenet-mini/train/n02480855/n02480855_15738.JPEG\n",
      "../data/imagenet-mini/train/n03379051/n03379051_7286.JPEG\n",
      "../data/imagenet-mini/train/n02977058/n02977058_22084.JPEG\n",
      "../data/imagenet-mini/train/n01773157/n01773157_9579.JPEG\n",
      "../data/imagenet-mini/train/n02088632/n02088632_4771.JPEG\n",
      "../data/imagenet-mini/train/n03709823/n03709823_1562.JPEG\n",
      "../data/imagenet-mini/train/n03530642/n03530642_46236.JPEG\n",
      "../data/imagenet-mini/train/n04579432/n04579432_18779.JPEG\n",
      "../data/imagenet-mini/train/n03976657/n03976657_8585.JPEG\n",
      "../data/imagenet-mini/train/n03841143/n03841143_2841.JPEG\n",
      "../data/imagenet-mini/train/n03995372/n03995372_14826.JPEG\n",
      "../data/imagenet-mini/train/n03995372/n03995372_3603.JPEG\n",
      "../data/imagenet-mini/train/n03325584/n03325584_10730.JPEG\n",
      "../data/imagenet-mini/train/n03216828/n03216828_28729.JPEG\n",
      "../data/imagenet-mini/train/n03216828/n03216828_60705.JPEG\n",
      "../data/imagenet-mini/train/n03216828/n03216828_984.JPEG\n",
      "../data/imagenet-mini/train/n03063599/n03063599_4132.JPEG\n",
      "../data/imagenet-mini/train/n04179913/n04179913_11903.JPEG\n",
      "../data/imagenet-mini/train/n04044716/n04044716_1626.JPEG\n",
      "../data/imagenet-mini/train/n04044716/n04044716_6855.JPEG\n",
      "../data/imagenet-mini/train/n02091032/n02091032_12363.JPEG\n",
      "../data/imagenet-mini/train/n03384352/n03384352_8614.JPEG\n",
      "../data/imagenet-mini/train/n03388549/n03388549_5594.JPEG\n",
      "../data/imagenet-mini/train/n04372370/n04372370_25321.JPEG\n",
      "../data/imagenet-mini/train/n04372370/n04372370_21436.JPEG\n",
      "../data/imagenet-mini/train/n04372370/n04372370_42250.JPEG\n",
      "../data/imagenet-mini/train/n03857828/n03857828_28253.JPEG\n",
      "../data/imagenet-mini/train/n03857828/n03857828_7955.JPEG\n",
      "../data/imagenet-mini/train/n04380533/n04380533_17646.JPEG\n",
      "../data/imagenet-mini/train/n04380533/n04380533_5780.JPEG\n",
      "../data/imagenet-mini/train/n04485082/n04485082_2236.JPEG\n",
      "../data/imagenet-mini/train/n04485082/n04485082_60909.JPEG\n",
      "../data/imagenet-mini/train/n04485082/n04485082_14560.JPEG\n",
      "../data/imagenet-mini/train/n03627232/n03627232_5560.JPEG\n",
      "../data/imagenet-mini/train/n04251144/n04251144_20993.JPEG\n",
      "../data/imagenet-mini/train/n04523525/n04523525_2111.JPEG\n",
      "BW cleaning terminated.\n"
     ]
    }
   ],
   "source": [
    "clean_bw_imgs(\"../bigdata/imagenet-mini/train/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to work on a subset of tinyimagenet for computations reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progression:95.00%\n",
      "Sampling terminated.\n"
     ]
    }
   ],
   "source": [
    "sample_imagefolder(\"../bigdata/imagenet-mini/train/\",\"../static/data/imagenet10classes/images\",num_dir=20,img_num_per_dir=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "# same preprocess used as vgg16\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.CenterCrop(224), \n",
    "    transforms.ToTensor()\n",
    "    ,transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell if you want to create the information for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json saving done!\n"
     ]
    }
   ],
   "source": [
    "#watch out: once you chose a folder name for the computed images and a json name, the json name will point to that folder name exclusively.\n",
    "lurker = Lurk(model,\n",
    "              preprocess,\n",
    "              labels_path=\"../bigdata/imagenet-mini/labels_imagenet.txt\",\n",
    "              save_gen_imgs_dir='../static/results/vgg16_imagenet/',\n",
    "              save_json_path='../static/saved_model/vgg16_imagenet.json',\n",
    "              imgs_src_dir=\"../static/data/imagenet10classes/images/\",\n",
    "              side_size=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to serve the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "lurker.serve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "lurker.end_serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to save/load the lurker in a convenient manner.. \\[OPTIONNAL\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-961f909d923d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlurker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_dill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../generated/vgg16_imagenet.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlurker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLurk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_dill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../generated/vgg16_imagenet.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3619090ba745>\u001b[0m in \u001b[0;36msave_to_dill\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_to_dill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dill saving done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mserialized_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_should_read_directly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lurker.save_to_dill(\"../generated/vgg16_imagenet.pickle\")\n",
    "\n",
    "\n",
    "lurker = Lurk.load_from_dill(\"../generated/vgg16_imagenet.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the average/maximum activation images from the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json saving done!\n"
     ]
    }
   ],
   "source": [
    "lurker.compute_top_imgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the \"most activating\" classes w.r.t a metric(max or avg) for a given filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lurker.plot_hist(0,0,\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibility to compute the layer visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization done!\n"
     ]
    }
   ],
   "source": [
    "lurker.compute_layer_viz(layer_indx = 0,filter_indexes=[6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the respective gradients w.r.t the most activating images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grads Progression:layer1/1 100.0%\n",
      "json saving done!\n"
     ]
    }
   ],
   "source": [
    "lurker.compute_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to load a precomputed json, just add the `load_path` attribute. Watch out, it needs to be coherent with the folder name.\\[OPTIONNAL\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading done!\n"
     ]
    }
   ],
   "source": [
    "lurker3 = Lurk(model,\n",
    "               preprocess,\n",
    "               labels_path=\"../bigdata/imagenet-mini/labels_imagenet.txt\",\n",
    "               save_gen_imgs_dir='../results/trash/',\n",
    "               save_json_path='../saved_model/new_vgg16.json',\n",
    "               imgs_src_dir=\"../data/imagenet10classes/images\",\n",
    "               load_json_path='../saved_model/vgg16_imagenet.json',\n",
    "               side_size=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Places365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: not available yet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download of the images possible on http://places2.csail.mit.edu/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_places = get_alex_places()\n",
    "\n",
    "places_preprocess = transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "lurker_alex_places = Lurk(alex_places\n",
    "                          ,places_preprocess\n",
    "                          ,labels_path=\"../data/places365/labels.txt\"\n",
    "                          ,save_gen_imgs_dir='../results/alex_places/'\n",
    "                          ,save_json_path='../saved_model/alex_places.json'\n",
    "                          ,imgs_src_dir=\"../data/places365/images/\"\n",
    "                          ,side_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#download the big dataset\n",
    "cifar_dataset = torchvision.datasets.CIFAR10(\"../bigdata/CIFAR10_dataset\", train=True, transform=None, target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progression:100.00 %\n"
     ]
    }
   ],
   "source": [
    "# takes ~ 5min\n",
    "#convert_to_jpg_dirs(cifar_dataset,\"../bigdata/CIFAR10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progression:90.00%\n",
      "Sampling terminated.\n"
     ]
    }
   ],
   "source": [
    "sample_imagefolder(\"../bigdata/CIFAR10/\",\"../data/CIFAR10/images\",num_dir=10,img_num_per_dir=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_labels(cifar_dataset.class_to_idx,\"../data/CIFAR10/labels_cifar.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_cif = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "model_cif = vgg.vgg11_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "lurker_cif = Lurk(model_cif,\n",
    "                  preprocess_cif,\n",
    "                  labels_path=\"../data/CIFAR10/labels_cifar.txt\",\n",
    "                  save_gen_imgs_dir='../results/cifar/',\n",
    "                  save_json_path='../saved_model/cifar.json',\n",
    "                  imgs_src_dir=\"../data/CIFAR10/images/\",\n",
    "                  side_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "lurker_cif.compute_top_imgs()\n",
    "\n",
    "lurker_cif.plot_hist(0,0,\"max\")\n",
    "\n",
    "lurker_cif.compute_layer_viz(layer_indx = 0,filter_indexes=[6,7])\n",
    "\n",
    "lurker_cif.compute_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Representation of a Lurk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML] *",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
